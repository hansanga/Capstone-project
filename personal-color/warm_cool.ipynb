{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from color_palette import PaletteCreator\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = PaletteCreator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('images/dataset/mean_shuffled.csv')\n",
    "df = pd.read_csv('images/dataset/mean_shuffled_wc.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "df['label'] = le.fit_transform(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>S</th>\n",
       "      <th>V</th>\n",
       "      <th>L</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>M</th>\n",
       "      <th>Y</th>\n",
       "      <th>K</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.333333</td>\n",
       "      <td>97.666667</td>\n",
       "      <td>157.666667</td>\n",
       "      <td>133.666667</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>142.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.844404</td>\n",
       "      <td>97.632279</td>\n",
       "      <td>0.381699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.666667</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>159.666667</td>\n",
       "      <td>140.666667</td>\n",
       "      <td>142.333333</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>71.246694</td>\n",
       "      <td>63.307304</td>\n",
       "      <td>0.373856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.333333</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>139.666667</td>\n",
       "      <td>139.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>54.558979</td>\n",
       "      <td>71.160311</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>88.333333</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>142.000000</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.094046</td>\n",
       "      <td>88.357131</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.666667</td>\n",
       "      <td>68.333333</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>140.333333</td>\n",
       "      <td>138.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>53.830104</td>\n",
       "      <td>68.238778</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>6.333333</td>\n",
       "      <td>103.666667</td>\n",
       "      <td>156.666667</td>\n",
       "      <td>128.666667</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>142.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>83.067656</td>\n",
       "      <td>103.661457</td>\n",
       "      <td>0.385621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>190.333333</td>\n",
       "      <td>167.333333</td>\n",
       "      <td>142.666667</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>56.608374</td>\n",
       "      <td>56.311671</td>\n",
       "      <td>0.253595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>156.333333</td>\n",
       "      <td>136.333333</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>57.940908</td>\n",
       "      <td>99.917318</td>\n",
       "      <td>0.386928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.333333</td>\n",
       "      <td>158.666667</td>\n",
       "      <td>131.333333</td>\n",
       "      <td>143.333333</td>\n",
       "      <td>143.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>74.628663</td>\n",
       "      <td>101.256745</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>9.333333</td>\n",
       "      <td>87.666667</td>\n",
       "      <td>162.666667</td>\n",
       "      <td>141.666667</td>\n",
       "      <td>139.333333</td>\n",
       "      <td>141.666667</td>\n",
       "      <td>0.0</td>\n",
       "      <td>61.292003</td>\n",
       "      <td>87.810297</td>\n",
       "      <td>0.362092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              H           S           V           L           A           B  \\\n",
       "0      8.333333   97.666667  157.666667  133.666667  142.000000  142.666667   \n",
       "1    118.666667   72.000000  159.666667  140.666667  142.333333  132.000000   \n",
       "2      7.333333   71.000000  158.000000  139.666667  139.000000  138.000000   \n",
       "3      4.666667   88.333333  170.000000  145.000000  142.000000  137.666667   \n",
       "4      6.666667   68.333333  177.000000  155.000000  140.333333  138.666667   \n",
       "..          ...         ...         ...         ...         ...         ...   \n",
       "397    6.333333  103.666667  156.666667  128.666667  145.000000  142.666667   \n",
       "398   60.000000   57.000000  190.333333  167.333333  142.666667  134.000000   \n",
       "399   13.000000  100.000000  156.333333  136.333333  137.666667  145.000000   \n",
       "400    8.000000  101.333333  158.666667  131.333333  143.333333  143.666667   \n",
       "401    9.333333   87.666667  162.666667  141.666667  139.333333  141.666667   \n",
       "\n",
       "       C          M           Y         K  label  \n",
       "0    0.0  70.844404   97.632279  0.381699      1  \n",
       "1    0.0  71.246694   63.307304  0.373856      0  \n",
       "2    0.0  54.558979   71.160311  0.380392      0  \n",
       "3    0.0  74.094046   88.357131  0.333333      1  \n",
       "4    0.0  53.830104   68.238778  0.305882      1  \n",
       "..   ...        ...         ...       ...    ...  \n",
       "397  0.0  83.067656  103.661457  0.385621      1  \n",
       "398  0.0  56.608374   56.311671  0.253595      0  \n",
       "399  0.0  57.940908   99.917318  0.386928      1  \n",
       "400  0.0  74.628663  101.256745  0.377778      1  \n",
       "401  0.0  61.292003   87.810297  0.362092      0  \n",
       "\n",
       "[402 rows x 11 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label    1.000000\n",
       "B        0.474724\n",
       "Y        0.473763\n",
       "S        0.469657\n",
       "M        0.293013\n",
       "K        0.116791\n",
       "A        0.097921\n",
       "V       -0.116791\n",
       "L       -0.206575\n",
       "H       -0.225074\n",
       "C             NaN\n",
       "Name: label, dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr['label'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['C'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k v a l\n",
    "df.drop(columns=['A'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>S</th>\n",
       "      <th>V</th>\n",
       "      <th>L</th>\n",
       "      <th>B</th>\n",
       "      <th>M</th>\n",
       "      <th>Y</th>\n",
       "      <th>K</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.333333</td>\n",
       "      <td>97.666667</td>\n",
       "      <td>157.666667</td>\n",
       "      <td>133.666667</td>\n",
       "      <td>142.666667</td>\n",
       "      <td>70.844404</td>\n",
       "      <td>97.632279</td>\n",
       "      <td>0.381699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>118.666667</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>159.666667</td>\n",
       "      <td>140.666667</td>\n",
       "      <td>132.000000</td>\n",
       "      <td>71.246694</td>\n",
       "      <td>63.307304</td>\n",
       "      <td>0.373856</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.333333</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>139.666667</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>54.558979</td>\n",
       "      <td>71.160311</td>\n",
       "      <td>0.380392</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.666667</td>\n",
       "      <td>88.333333</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>137.666667</td>\n",
       "      <td>74.094046</td>\n",
       "      <td>88.357131</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6.666667</td>\n",
       "      <td>68.333333</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>138.666667</td>\n",
       "      <td>53.830104</td>\n",
       "      <td>68.238778</td>\n",
       "      <td>0.305882</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>6.333333</td>\n",
       "      <td>103.666667</td>\n",
       "      <td>156.666667</td>\n",
       "      <td>128.666667</td>\n",
       "      <td>142.666667</td>\n",
       "      <td>83.067656</td>\n",
       "      <td>103.661457</td>\n",
       "      <td>0.385621</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>398</th>\n",
       "      <td>60.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>190.333333</td>\n",
       "      <td>167.333333</td>\n",
       "      <td>134.000000</td>\n",
       "      <td>56.608374</td>\n",
       "      <td>56.311671</td>\n",
       "      <td>0.253595</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>399</th>\n",
       "      <td>13.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>156.333333</td>\n",
       "      <td>136.333333</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>57.940908</td>\n",
       "      <td>99.917318</td>\n",
       "      <td>0.386928</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>8.000000</td>\n",
       "      <td>101.333333</td>\n",
       "      <td>158.666667</td>\n",
       "      <td>131.333333</td>\n",
       "      <td>143.666667</td>\n",
       "      <td>74.628663</td>\n",
       "      <td>101.256745</td>\n",
       "      <td>0.377778</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>9.333333</td>\n",
       "      <td>87.666667</td>\n",
       "      <td>162.666667</td>\n",
       "      <td>141.666667</td>\n",
       "      <td>141.666667</td>\n",
       "      <td>61.292003</td>\n",
       "      <td>87.810297</td>\n",
       "      <td>0.362092</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>402 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              H           S           V           L           B          M  \\\n",
       "0      8.333333   97.666667  157.666667  133.666667  142.666667  70.844404   \n",
       "1    118.666667   72.000000  159.666667  140.666667  132.000000  71.246694   \n",
       "2      7.333333   71.000000  158.000000  139.666667  138.000000  54.558979   \n",
       "3      4.666667   88.333333  170.000000  145.000000  137.666667  74.094046   \n",
       "4      6.666667   68.333333  177.000000  155.000000  138.666667  53.830104   \n",
       "..          ...         ...         ...         ...         ...        ...   \n",
       "397    6.333333  103.666667  156.666667  128.666667  142.666667  83.067656   \n",
       "398   60.000000   57.000000  190.333333  167.333333  134.000000  56.608374   \n",
       "399   13.000000  100.000000  156.333333  136.333333  145.000000  57.940908   \n",
       "400    8.000000  101.333333  158.666667  131.333333  143.666667  74.628663   \n",
       "401    9.333333   87.666667  162.666667  141.666667  141.666667  61.292003   \n",
       "\n",
       "              Y         K  label  \n",
       "0     97.632279  0.381699      1  \n",
       "1     63.307304  0.373856      0  \n",
       "2     71.160311  0.380392      0  \n",
       "3     88.357131  0.333333      1  \n",
       "4     68.238778  0.305882      1  \n",
       "..          ...       ...    ...  \n",
       "397  103.661457  0.385621      1  \n",
       "398   56.311671  0.253595      0  \n",
       "399   99.917318  0.386928      1  \n",
       "400  101.256745  0.377778      1  \n",
       "401   87.810297  0.362092      0  \n",
       "\n",
       "[402 rows x 9 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label'])\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(logreg, X, Y, verbose=1, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7115123456790122"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn1 = KNeighborsClassifier(n_neighbors=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    }
   ],
   "source": [
    "score = cross_val_score(knn1, X, Y, verbose=1, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6816049382716051"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn2 = KNeighborsClassifier(n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6916975308641975"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(knn2, X, Y, verbose=1, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd1 = SGDClassifier(loss='hinge', verbose=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 41.71, NNZs: 8, Bias: -9.087102, T: 321, Avg. loss: 12.554406\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 33.73, NNZs: 8, Bias: -1.347439, T: 642, Avg. loss: 10.082637\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 46.20, NNZs: 8, Bias: -1.690389, T: 963, Avg. loss: 9.282107\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.65, NNZs: 8, Bias: 7.297193, T: 1284, Avg. loss: 6.874207\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 33.01, NNZs: 8, Bias: -5.334008, T: 1605, Avg. loss: 5.449763\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 37.38, NNZs: 8, Bias: -1.705214, T: 1926, Avg. loss: 5.562918\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 34.59, NNZs: 8, Bias: -1.725273, T: 2247, Avg. loss: 3.965719\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 27.71, NNZs: 8, Bias: -7.281537, T: 2568, Avg. loss: 4.657190\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 30.60, NNZs: 8, Bias: -4.169337, T: 2889, Avg. loss: 3.760619\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 29.51, NNZs: 8, Bias: 3.072137, T: 3210, Avg. loss: 3.754121\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 32.01, NNZs: 8, Bias: 4.945197, T: 3531, Avg. loss: 3.393875\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 29.14, NNZs: 8, Bias: 2.553422, T: 3852, Avg. loss: 3.575135\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 28.05, NNZs: 8, Bias: -3.332931, T: 4173, Avg. loss: 3.220131\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 26.26, NNZs: 8, Bias: -4.981437, T: 4494, Avg. loss: 3.023981\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 25.72, NNZs: 8, Bias: 2.115314, T: 4815, Avg. loss: 2.756117\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 24.54, NNZs: 8, Bias: -1.338763, T: 5136, Avg. loss: 2.896052\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 24.27, NNZs: 8, Bias: -4.343917, T: 5457, Avg. loss: 2.601595\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 24.28, NNZs: 8, Bias: 3.185591, T: 5778, Avg. loss: 2.173272\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 24.21, NNZs: 8, Bias: -1.210841, T: 6099, Avg. loss: 2.312972\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 23.60, NNZs: 8, Bias: -1.187441, T: 6420, Avg. loss: 2.226432\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 21.90, NNZs: 8, Bias: 2.721994, T: 6741, Avg. loss: 1.881886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 21.09, NNZs: 8, Bias: 0.093722, T: 7062, Avg. loss: 2.195233\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 20.17, NNZs: 8, Bias: -2.344233, T: 7383, Avg. loss: 1.891688\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 19.96, NNZs: 8, Bias: -1.108867, T: 7704, Avg. loss: 2.259353\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 18.11, NNZs: 8, Bias: -2.200698, T: 8025, Avg. loss: 2.025637\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 18.99, NNZs: 8, Bias: 1.068034, T: 8346, Avg. loss: 1.714145\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 18.84, NNZs: 8, Bias: 1.027918, T: 8667, Avg. loss: 1.773473\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 17.96, NNZs: 8, Bias: -4.015871, T: 8988, Avg. loss: 1.978522\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 17.70, NNZs: 8, Bias: -0.993731, T: 9309, Avg. loss: 2.015623\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 17.95, NNZs: 8, Bias: -0.035297, T: 9630, Avg. loss: 1.834625\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 17.93, NNZs: 8, Bias: -0.985631, T: 9951, Avg. loss: 1.596218\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 17.28, NNZs: 8, Bias: -0.084253, T: 10272, Avg. loss: 1.700702\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 17.44, NNZs: 8, Bias: -0.961114, T: 10593, Avg. loss: 1.449289\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 16.42, NNZs: 8, Bias: 0.729103, T: 10914, Avg. loss: 1.716533\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 17.02, NNZs: 8, Bias: 0.701179, T: 11235, Avg. loss: 1.610795\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 15.87, NNZs: 8, Bias: 0.673914, T: 11556, Avg. loss: 1.716731\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 16.47, NNZs: 8, Bias: -3.243989, T: 11877, Avg. loss: 1.497653\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 16.15, NNZs: 8, Bias: -0.145327, T: 12198, Avg. loss: 1.758920\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 38 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 24.71, NNZs: 8, Bias: -8.801036, T: 321, Avg. loss: 13.875366\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 27.30, NNZs: 8, Bias: -7.687662, T: 642, Avg. loss: 9.942050\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 24.28, NNZs: 8, Bias: 13.175065, T: 963, Avg. loss: 8.593481\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 23.37, NNZs: 8, Bias: -10.354819, T: 1284, Avg. loss: 7.240406\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 26.13, NNZs: 8, Bias: -1.609770, T: 1605, Avg. loss: 5.807928\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 26.86, NNZs: 8, Bias: -1.406714, T: 1926, Avg. loss: 6.128637\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 26.62, NNZs: 8, Bias: 1.643261, T: 2247, Avg. loss: 4.749221\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 26.04, NNZs: 8, Bias: -6.885553, T: 2568, Avg. loss: 4.688474\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 22.74, NNZs: 8, Bias: -3.999730, T: 2889, Avg. loss: 4.142258\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 20.47, NNZs: 8, Bias: 1.007375, T: 3210, Avg. loss: 4.000813\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 20.53, NNZs: 8, Bias: 2.959193, T: 3531, Avg. loss: 3.564569\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 17.02, NNZs: 8, Bias: 0.666516, T: 3852, Avg. loss: 3.837067\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 15.33, NNZs: 8, Bias: -5.263696, T: 4173, Avg. loss: 3.260843\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 16.49, NNZs: 8, Bias: -1.375763, T: 4494, Avg. loss: 3.301537\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 18.12, NNZs: 8, Bias: 3.859930, T: 4815, Avg. loss: 2.952390\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 17.88, NNZs: 8, Bias: -2.941918, T: 5136, Avg. loss: 2.577639\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 13.72, NNZs: 8, Bias: -2.800332, T: 5457, Avg. loss: 2.511261\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 15.65, NNZs: 8, Bias: 1.731775, T: 5778, Avg. loss: 2.340918\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 14.88, NNZs: 8, Bias: -2.586931, T: 6099, Avg. loss: 2.411601\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 15.10, NNZs: 8, Bias: 0.187482, T: 6420, Avg. loss: 2.615719\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 14.16, NNZs: 8, Bias: 0.188719, T: 6741, Avg. loss: 2.465290\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 14.75, NNZs: 8, Bias: -1.114476, T: 7062, Avg. loss: 2.426778\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 13.69, NNZs: 8, Bias: 1.322127, T: 7383, Avg. loss: 1.945949\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 12.93, NNZs: 8, Bias: 4.712600, T: 7704, Avg. loss: 2.190506\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 11.29, NNZs: 8, Bias: -2.113945, T: 8025, Avg. loss: 2.294900\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 12.21, NNZs: 8, Bias: -0.971937, T: 8346, Avg. loss: 1.849020\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 13.80, NNZs: 8, Bias: 1.131575, T: 8667, Avg. loss: 1.748453\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 11.49, NNZs: 8, Bias: -1.911725, T: 8988, Avg. loss: 2.085425\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 11.06, NNZs: 8, Bias: 0.102611, T: 9309, Avg. loss: 1.979367\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 9.92, NNZs: 8, Bias: 0.096140, T: 9630, Avg. loss: 1.979150\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 9.65, NNZs: 8, Bias: -0.826895, T: 9951, Avg. loss: 1.789513\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 8.64, NNZs: 8, Bias: 0.952205, T: 10272, Avg. loss: 1.865332\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 32 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 25.52, NNZs: 8, Bias: -16.332630, T: 322, Avg. loss: 12.465327\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 33.94, NNZs: 8, Bias: 10.310769, T: 644, Avg. loss: 11.199525\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 45.36, NNZs: 8, Bias: -2.037654, T: 966, Avg. loss: 8.417439\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 39.55, NNZs: 8, Bias: -15.343568, T: 1288, Avg. loss: 8.559946\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 42.72, NNZs: 8, Bias: -17.705270, T: 1610, Avg. loss: 6.020264\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 35.83, NNZs: 8, Bias: 11.293457, T: 1932, Avg. loss: 4.698421\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 29.13, NNZs: 8, Bias: 0.692521, T: 2254, Avg. loss: 4.872937\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 29.20, NNZs: 8, Bias: 0.657629, T: 2576, Avg. loss: 4.585989\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 26.87, NNZs: 8, Bias: -4.568325, T: 2898, Avg. loss: 4.243233\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 30.06, NNZs: 8, Bias: -1.990387, T: 3220, Avg. loss: 3.999360\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 25.69, NNZs: 8, Bias: -4.041560, T: 3542, Avg. loss: 3.643837\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 24.31, NNZs: 8, Bias: 4.451579, T: 3864, Avg. loss: 3.822575\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 23.77, NNZs: 8, Bias: -3.535978, T: 4186, Avg. loss: 3.450642\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 22.97, NNZs: 8, Bias: -3.404597, T: 4508, Avg. loss: 3.189582\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 20.16, NNZs: 8, Bias: 0.212716, T: 4830, Avg. loss: 3.151899\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 19.30, NNZs: 8, Bias: 3.425314, T: 5152, Avg. loss: 3.108012\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 17.51, NNZs: 8, Bias: -1.417981, T: 5474, Avg. loss: 2.798050\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 19.66, NNZs: 8, Bias: 6.011594, T: 5796, Avg. loss: 3.025089\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 21.12, NNZs: 8, Bias: -2.700903, T: 6118, Avg. loss: 2.394944\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 21.74, NNZs: 8, Bias: 2.813127, T: 6440, Avg. loss: 2.587057\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 21.01, NNZs: 8, Bias: 1.383265, T: 6762, Avg. loss: 2.341886\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 21.88, NNZs: 8, Bias: -2.330466, T: 7084, Avg. loss: 2.528177\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 21.91, NNZs: 8, Bias: 3.713072, T: 7406, Avg. loss: 2.229983\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 20.48, NNZs: 8, Bias: 0.117243, T: 7728, Avg. loss: 2.343364\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 19.97, NNZs: 8, Bias: -5.438574, T: 8050, Avg. loss: 2.291749\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 18.16, NNZs: 8, Bias: 0.087147, T: 8372, Avg. loss: 2.215427\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 17.15, NNZs: 8, Bias: -1.960469, T: 8694, Avg. loss: 1.823925\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 17.06, NNZs: 8, Bias: 1.087394, T: 9016, Avg. loss: 2.131936\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 16.33, NNZs: 8, Bias: 0.080498, T: 9338, Avg. loss: 1.855975\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 16.79, NNZs: 8, Bias: 1.955919, T: 9660, Avg. loss: 1.534853\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 16.14, NNZs: 8, Bias: -2.655058, T: 9982, Avg. loss: 2.050516\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 15.08, NNZs: 8, Bias: -1.720164, T: 10304, Avg. loss: 2.030626\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 15.16, NNZs: 8, Bias: 0.891567, T: 10626, Avg. loss: 1.777261\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 16.20, NNZs: 8, Bias: -0.789203, T: 10948, Avg. loss: 1.918295\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 16.33, NNZs: 8, Bias: 1.669053, T: 11270, Avg. loss: 1.629601\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 35 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 28.68, NNZs: 8, Bias: -22.987015, T: 322, Avg. loss: 13.839236\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 41.45, NNZs: 8, Bias: -12.867187, T: 644, Avg. loss: 10.409584\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 34.18, NNZs: 8, Bias: -0.890493, T: 966, Avg. loss: 8.309862\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 29.01, NNZs: 8, Bias: -9.542515, T: 1288, Avg. loss: 6.640256\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 36.05, NNZs: 8, Bias: -12.680901, T: 1610, Avg. loss: 5.409399\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 29.47, NNZs: 8, Bias: 2.224934, T: 1932, Avg. loss: 5.188777\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 31.96, NNZs: 8, Bias: -1.123651, T: 2254, Avg. loss: 4.415782\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 23.04, NNZs: 8, Bias: 4.554336, T: 2576, Avg. loss: 4.195484\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 23.66, NNZs: 8, Bias: -0.969142, T: 2898, Avg. loss: 4.000196\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 20.67, NNZs: 8, Bias: -3.384249, T: 3220, Avg. loss: 3.551653\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 22.40, NNZs: 8, Bias: -1.148453, T: 3542, Avg. loss: 2.923592\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 21.87, NNZs: 8, Bias: 3.044479, T: 3864, Avg. loss: 3.577034\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 20.87, NNZs: 8, Bias: 2.802705, T: 4186, Avg. loss: 3.188299\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 21.84, NNZs: 8, Bias: -2.825218, T: 4508, Avg. loss: 2.759100\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 20.38, NNZs: 8, Bias: 2.383939, T: 4830, Avg. loss: 2.906687\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 19.26, NNZs: 8, Bias: 3.862639, T: 5152, Avg. loss: 2.860930\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 17.36, NNZs: 8, Bias: -4.084023, T: 5474, Avg. loss: 2.525523\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 15.91, NNZs: 8, Bias: 1.943490, T: 5796, Avg. loss: 2.415948\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 17.32, NNZs: 8, Bias: -2.362039, T: 6118, Avg. loss: 2.438754\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 18.13, NNZs: 8, Bias: -2.290818, T: 6440, Avg. loss: 2.245576\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 16.95, NNZs: 8, Bias: 0.341994, T: 6762, Avg. loss: 2.453891\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 16.23, NNZs: 8, Bias: -2.159678, T: 7084, Avg. loss: 2.407213\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 15.95, NNZs: 8, Bias: 0.273772, T: 7406, Avg. loss: 2.014955\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 16.90, NNZs: 8, Bias: -2.048575, T: 7728, Avg. loss: 2.020502\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 15.86, NNZs: 8, Bias: -0.872110, T: 8050, Avg. loss: 2.048390\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 14.43, NNZs: 8, Bias: -1.920359, T: 8372, Avg. loss: 1.880598\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 14.67, NNZs: 8, Bias: -3.935342, T: 8694, Avg. loss: 1.814070\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 15.02, NNZs: 8, Bias: -1.827419, T: 9016, Avg. loss: 1.808459\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 13.00, NNZs: 8, Bias: 0.143234, T: 9338, Avg. loss: 1.905432\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 13.26, NNZs: 8, Bias: 0.136333, T: 9660, Avg. loss: 1.615062\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 15.16, NNZs: 8, Bias: -1.697226, T: 9982, Avg. loss: 1.820975\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 13.89, NNZs: 8, Bias: -0.785018, T: 10304, Avg. loss: 1.794133\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 12.29, NNZs: 8, Bias: 1.796417, T: 10626, Avg. loss: 1.513360\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 12.97, NNZs: 8, Bias: -0.748407, T: 10948, Avg. loss: 1.623177\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 13.27, NNZs: 8, Bias: 0.071634, T: 11270, Avg. loss: 1.471599\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 12.46, NNZs: 8, Bias: -2.325920, T: 11592, Avg. loss: 1.560970\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 10.88, NNZs: 8, Bias: 0.814277, T: 11914, Avg. loss: 1.484656\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 11.59, NNZs: 8, Bias: 0.026205, T: 12236, Avg. loss: 1.597100\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 11.90, NNZs: 8, Bias: 1.502325, T: 12558, Avg. loss: 1.489116\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 11.55, NNZs: 8, Bias: -0.710121, T: 12880, Avg. loss: 1.404358\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 11.11, NNZs: 8, Bias: 2.124228, T: 13202, Avg. loss: 1.435115\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 12.41, NNZs: 8, Bias: -0.668927, T: 13524, Avg. loss: 1.469282\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 11.72, NNZs: 8, Bias: -1.331576, T: 13846, Avg. loss: 1.515179\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 11.46, NNZs: 8, Bias: -1.974739, T: 14168, Avg. loss: 1.237503\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 12.12, NNZs: 8, Bias: -0.643047, T: 14490, Avg. loss: 1.236110\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 11.53, NNZs: 8, Bias: 0.627170, T: 14812, Avg. loss: 1.300083\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 11.24, NNZs: 8, Bias: -0.009792, T: 15134, Avg. loss: 1.278249\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 11.55, NNZs: 8, Bias: 0.595366, T: 15456, Avg. loss: 1.254125\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 11.74, NNZs: 8, Bias: -1.204840, T: 15778, Avg. loss: 1.406111\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 11.04, NNZs: 8, Bias: -1.773152, T: 16100, Avg. loss: 1.190537\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 10.30, NNZs: 8, Bias: -0.604990, T: 16422, Avg. loss: 1.296014\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 10.14, NNZs: 8, Bias: 0.536576, T: 16744, Avg. loss: 1.257776\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 10.19, NNZs: 8, Bias: -0.032151, T: 17066, Avg. loss: 1.294863\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 9.64, NNZs: 8, Bias: -0.035396, T: 17388, Avg. loss: 1.151466\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 9.73, NNZs: 8, Bias: -0.570349, T: 17710, Avg. loss: 1.208002\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 8.82, NNZs: 8, Bias: 0.486749, T: 18032, Avg. loss: 1.154953\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 8.37, NNZs: 8, Bias: -0.559508, T: 18354, Avg. loss: 1.294201\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 9.08, NNZs: 8, Bias: 0.977663, T: 18676, Avg. loss: 1.192031\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 9.05, NNZs: 8, Bias: 0.456348, T: 18998, Avg. loss: 1.283029\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 59 epochs took 0.00 seconds\n",
      "-- Epoch 1\n",
      "Norm: 24.70, NNZs: 8, Bias: 0.531347, T: 322, Avg. loss: 13.720331\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 38.46, NNZs: 8, Bias: -11.534929, T: 644, Avg. loss: 10.761597\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 27.47, NNZs: 8, Bias: 0.097319, T: 966, Avg. loss: 8.978771\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 36.85, NNZs: 8, Bias: -17.281136, T: 1288, Avg. loss: 7.325560\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 28.93, NNZs: 8, Bias: -15.184439, T: 1610, Avg. loss: 6.142217\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 32.19, NNZs: 8, Bias: -3.390963, T: 1932, Avg. loss: 5.593568\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 26.15, NNZs: 8, Bias: -0.156248, T: 2254, Avg. loss: 5.508599\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 28.81, NNZs: 8, Bias: 5.528350, T: 2576, Avg. loss: 4.174257\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 28.89, NNZs: 8, Bias: 2.480232, T: 2898, Avg. loss: 4.551745\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 27.79, NNZs: 8, Bias: -2.458810, T: 3220, Avg. loss: 3.658424\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 25.13, NNZs: 8, Bias: -0.015768, T: 3542, Avg. loss: 3.599630\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 26.04, NNZs: 8, Bias: 2.051119, T: 3864, Avg. loss: 3.413851\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 24.99, NNZs: 8, Bias: 0.024544, T: 4186, Avg. loss: 3.534610\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 27.01, NNZs: 8, Bias: 1.842550, T: 4508, Avg. loss: 3.142980\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 25.18, NNZs: 8, Bias: 1.708596, T: 4830, Avg. loss: 2.895793\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 24.35, NNZs: 8, Bias: 4.901065, T: 5152, Avg. loss: 3.220740\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 22.26, NNZs: 8, Bias: 0.042463, T: 5474, Avg. loss: 3.106661\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 22.74, NNZs: 8, Bias: 0.030544, T: 5796, Avg. loss: 2.785629\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 19.93, NNZs: 8, Bias: -0.016911, T: 6118, Avg. loss: 2.616131\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 20.51, NNZs: 8, Bias: -1.367432, T: 6440, Avg. loss: 2.397190\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 19.37, NNZs: 8, Bias: -1.340382, T: 6762, Avg. loss: 2.442018\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 19.72, NNZs: 8, Bias: -1.300041, T: 7084, Avg. loss: 2.166802\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 20.23, NNZs: 8, Bias: 3.514245, T: 7406, Avg. loss: 2.418420\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 24\n",
      "Norm: 19.52, NNZs: 8, Bias: -0.048246, T: 7728, Avg. loss: 2.260485\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 25\n",
      "Norm: 18.53, NNZs: 8, Bias: -2.254513, T: 8050, Avg. loss: 2.176865\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 26\n",
      "Norm: 18.18, NNZs: 8, Bias: -3.276457, T: 8372, Avg. loss: 2.001593\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 27\n",
      "Norm: 17.32, NNZs: 8, Bias: 0.943314, T: 8694, Avg. loss: 1.957472\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 28\n",
      "Norm: 17.49, NNZs: 8, Bias: -1.078355, T: 9016, Avg. loss: 2.014293\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 29\n",
      "Norm: 17.82, NNZs: 8, Bias: 1.859045, T: 9338, Avg. loss: 1.833650\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 30\n",
      "Norm: 16.55, NNZs: 8, Bias: 2.755001, T: 9660, Avg. loss: 2.031396\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 31\n",
      "Norm: 16.74, NNZs: 8, Bias: -0.961901, T: 9982, Avg. loss: 1.876343\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 32\n",
      "Norm: 17.35, NNZs: 8, Bias: -0.060099, T: 10304, Avg. loss: 1.717564\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 33\n",
      "Norm: 17.99, NNZs: 8, Bias: -1.792156, T: 10626, Avg. loss: 1.754241\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 34\n",
      "Norm: 16.53, NNZs: 8, Bias: -0.064814, T: 10948, Avg. loss: 1.882088\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 35\n",
      "Norm: 16.20, NNZs: 8, Bias: -0.876119, T: 11270, Avg. loss: 1.716753\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 36\n",
      "Norm: 16.10, NNZs: 8, Bias: -0.071308, T: 11592, Avg. loss: 1.713510\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 37\n",
      "Norm: 15.86, NNZs: 8, Bias: -0.081150, T: 11914, Avg. loss: 1.681982\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 38\n",
      "Norm: 16.10, NNZs: 8, Bias: -0.825398, T: 12236, Avg. loss: 1.526141\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 39\n",
      "Norm: 16.25, NNZs: 8, Bias: 0.676732, T: 12558, Avg. loss: 1.629670\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 40\n",
      "Norm: 15.57, NNZs: 8, Bias: -0.060478, T: 12880, Avg. loss: 1.637989\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 41\n",
      "Norm: 15.41, NNZs: 8, Bias: 2.058334, T: 13202, Avg. loss: 1.603017\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 42\n",
      "Norm: 15.42, NNZs: 8, Bias: 0.640706, T: 13524, Avg. loss: 1.560370\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 43\n",
      "Norm: 15.61, NNZs: 8, Bias: -0.712553, T: 13846, Avg. loss: 1.441145\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 44\n",
      "Norm: 14.92, NNZs: 8, Bias: -1.367627, T: 14168, Avg. loss: 1.447428\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 45\n",
      "Norm: 14.99, NNZs: 8, Bias: -0.046298, T: 14490, Avg. loss: 1.409340\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 46\n",
      "Norm: 14.99, NNZs: 8, Bias: -0.673978, T: 14812, Avg. loss: 1.361687\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 47\n",
      "Norm: 15.35, NNZs: 8, Bias: -0.664822, T: 15134, Avg. loss: 1.435900\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 48\n",
      "Norm: 15.16, NNZs: 8, Bias: -1.263514, T: 15456, Avg. loss: 1.458619\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 49\n",
      "Norm: 14.93, NNZs: 8, Bias: -0.651824, T: 15778, Avg. loss: 1.448382\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 50\n",
      "Norm: 14.06, NNZs: 8, Bias: -0.641649, T: 16100, Avg. loss: 1.329079\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 51\n",
      "Norm: 13.94, NNZs: 8, Bias: -1.781514, T: 16422, Avg. loss: 1.454521\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 52\n",
      "Norm: 13.92, NNZs: 8, Bias: 0.504704, T: 16744, Avg. loss: 1.452311\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 53\n",
      "Norm: 13.76, NNZs: 8, Bias: -0.053385, T: 17066, Avg. loss: 1.517050\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 54\n",
      "Norm: 13.59, NNZs: 8, Bias: 0.492506, T: 17388, Avg. loss: 1.376608\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 55\n",
      "Norm: 13.40, NNZs: 8, Bias: -0.054383, T: 17710, Avg. loss: 1.281867\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 56\n",
      "Norm: 12.78, NNZs: 8, Bias: -0.581164, T: 18032, Avg. loss: 1.177165\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 57\n",
      "Norm: 12.70, NNZs: 8, Bias: 0.985685, T: 18354, Avg. loss: 1.305079\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 58\n",
      "Norm: 12.71, NNZs: 8, Bias: 0.971819, T: 18676, Avg. loss: 1.213666\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 59\n",
      "Norm: 12.78, NNZs: 8, Bias: 1.460956, T: 18998, Avg. loss: 1.296781\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 60\n",
      "Norm: 12.44, NNZs: 8, Bias: 0.942886, T: 19320, Avg. loss: 1.364680\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 61\n",
      "Norm: 13.11, NNZs: 8, Bias: 0.444278, T: 19642, Avg. loss: 1.223916\n",
      "Total training time: 0.00 seconds.\n",
      "Convergence after 61 epochs took 0.00 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6816358024691358"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(sgd1, X, Y, verbose=1, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc1 = SVC(kernel='rbf', C=100, probability=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6442901234567902"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(svc1, X, Y, verbose=1, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc2 = SVC(kernel='rbf', C=10, probability=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6991049382716049"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(svc2, X, Y, verbose=1, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc3 = SVC(random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   5 out of   5 | elapsed:    0.0s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6966049382716049"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(svc3, X, Y, verbose=1, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = ['linear', 'poly', 'sigmoid', 'rbf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear 0.1 0.6941666666666666\n",
      "linear 1 0.7065432098765433\n",
      "linear 10 0.7139506172839507\n",
      "linear 100 0.7064506172839506\n",
      "poly 0.1 0.6693827160493827\n",
      "poly 1 0.6867592592592592\n",
      "poly 10 0.6618518518518519\n",
      "poly 100 0.6493518518518518\n",
      "sigmoid 0.1 0.6839814814814815\n",
      "sigmoid 1 0.5399074074074074\n",
      "sigmoid 10 0.5473456790123457\n",
      "sigmoid 100 0.5473456790123457\n",
      "rbf 0.1 0.6890740740740741\n",
      "rbf 1 0.6966049382716049\n",
      "rbf 10 0.6991049382716049\n",
      "rbf 100 0.6442901234567902\n"
     ]
    }
   ],
   "source": [
    "for k in kernel:\n",
    "    for c in [0.1, 1, 10, 100]:\n",
    "        svc = SVC(kernel=k, C=c, probability=True, random_state=42)\n",
    "        score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "        print(f'{k} {c} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear 1 0.6941666666666666\n",
      "linear 2 0.7016666666666668\n",
      "linear 3 0.7091049382716049\n",
      "linear 4 0.7140740740740741\n",
      "linear 5 0.7190123456790123\n",
      "linear 6 0.7190123456790123\n",
      "linear 7 0.7140432098765432\n",
      "linear 8 0.7140432098765432\n",
      "poly 1 0.6693827160493827\n",
      "poly 2 0.6618518518518519\n",
      "poly 3 0.6619135802469136\n",
      "poly 4 0.6693209876543209\n",
      "poly 5 0.6643518518518519\n",
      "poly 6 0.6718827160493827\n",
      "poly 7 0.6693827160493827\n",
      "poly 8 0.6694135802469136\n",
      "sigmoid 1 0.6839814814814815\n",
      "sigmoid 2 0.6218209876543209\n",
      "sigmoid 3 0.5720679012345679\n",
      "sigmoid 4 0.5523456790123457\n",
      "sigmoid 5 0.5424074074074074\n",
      "sigmoid 6 0.5448765432098764\n",
      "sigmoid 7 0.5399074074074075\n",
      "sigmoid 8 0.5399074074074075\n",
      "rbf 1 0.6890740740740741\n",
      "rbf 2 0.6867592592592593\n",
      "rbf 3 0.6966358024691358\n",
      "rbf 4 0.6891358024691359\n",
      "rbf 5 0.6891666666666667\n",
      "rbf 6 0.6891975308641975\n",
      "rbf 7 0.6966358024691358\n",
      "rbf 8 0.6941358024691358\n"
     ]
    }
   ],
   "source": [
    "for k in kernel:\n",
    "    for c in range(1, 9):\n",
    "        svc = SVC(kernel=k, C=(c * .1), probability=True, random_state=42)\n",
    "        score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "        print(f'{k} {c} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = [0.01, 0.1, 1, 10, 50, 100, 'auto', 'scale']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linear 1 0.01 0.7065432098765433\n",
      "linear 1 0.01 0.6941666666666666\n",
      "linear 1 0.01 0.7139506172839507\n",
      "linear 1 0.1 0.7065432098765433\n",
      "linear 1 0.1 0.6941666666666666\n",
      "linear 1 0.1 0.7139506172839507\n",
      "linear 1 1 0.7065432098765433\n",
      "linear 1 1 0.6941666666666666\n",
      "linear 1 1 0.7139506172839507\n",
      "linear 1 10 0.7065432098765433\n",
      "linear 1 10 0.6941666666666666\n",
      "linear 1 10 0.7139506172839507\n",
      "linear 1 50 0.7065432098765433\n",
      "linear 1 50 0.6941666666666666\n",
      "linear 1 50 0.7139506172839507\n",
      "linear 1 100 0.7065432098765433\n",
      "linear 1 100 0.6941666666666666\n",
      "linear 1 100 0.7139506172839507\n",
      "linear 1 auto 0.7065432098765433\n",
      "linear 1 auto 0.6941666666666666\n",
      "linear 1 auto 0.7139506172839507\n",
      "linear 1 scale 0.7065432098765433\n",
      "linear 1 scale 0.6941666666666666\n",
      "linear 1 scale 0.7139506172839507\n",
      "linear 2 0.01 0.7090123456790124\n",
      "linear 2 0.01 0.7016666666666668\n",
      "linear 2 0.01 0.7090123456790124\n",
      "linear 2 0.1 0.7090123456790124\n",
      "linear 2 0.1 0.7016666666666668\n",
      "linear 2 0.1 0.7090123456790124\n",
      "linear 2 1 0.7090123456790124\n",
      "linear 2 1 0.7016666666666668\n",
      "linear 2 1 0.7090123456790124\n",
      "linear 2 10 0.7090123456790124\n",
      "linear 2 10 0.7016666666666668\n",
      "linear 2 10 0.7090123456790124\n",
      "linear 2 50 0.7090123456790124\n",
      "linear 2 50 0.7016666666666668\n",
      "linear 2 50 0.7090123456790124\n",
      "linear 2 100 0.7090123456790124\n",
      "linear 2 100 0.7016666666666668\n",
      "linear 2 100 0.7090123456790124\n",
      "linear 2 auto 0.7090123456790124\n",
      "linear 2 auto 0.7016666666666668\n",
      "linear 2 auto 0.7090123456790124\n",
      "linear 2 scale 0.7090123456790124\n",
      "linear 2 scale 0.7016666666666668\n",
      "linear 2 scale 0.7090123456790124\n",
      "linear 3 0.01 0.7065432098765433\n",
      "linear 3 0.01 0.7091049382716049\n",
      "linear 3 0.01 0.7065123456790123\n",
      "linear 3 0.1 0.7065432098765433\n",
      "linear 3 0.1 0.7091049382716049\n",
      "linear 3 0.1 0.7065123456790123\n",
      "linear 3 1 0.7065432098765433\n",
      "linear 3 1 0.7091049382716049\n",
      "linear 3 1 0.7065123456790123\n",
      "linear 3 10 0.7065432098765433\n",
      "linear 3 10 0.7091049382716049\n",
      "linear 3 10 0.7065123456790123\n",
      "linear 3 50 0.7065432098765433\n",
      "linear 3 50 0.7091049382716049\n",
      "linear 3 50 0.7065123456790123\n",
      "linear 3 100 0.7065432098765433\n",
      "linear 3 100 0.7091049382716049\n",
      "linear 3 100 0.7065123456790123\n",
      "linear 3 auto 0.7065432098765433\n",
      "linear 3 auto 0.7091049382716049\n",
      "linear 3 auto 0.7065123456790123\n",
      "linear 3 scale 0.7065432098765433\n",
      "linear 3 scale 0.7091049382716049\n",
      "linear 3 scale 0.7065123456790123\n",
      "linear 4 0.01 0.7040432098765432\n",
      "linear 4 0.01 0.7140740740740741\n",
      "linear 4 0.01 0.7114506172839506\n",
      "linear 4 0.1 0.7040432098765432\n",
      "linear 4 0.1 0.7140740740740741\n",
      "linear 4 0.1 0.7114506172839506\n",
      "linear 4 1 0.7040432098765432\n",
      "linear 4 1 0.7140740740740741\n",
      "linear 4 1 0.7114506172839506\n",
      "linear 4 10 0.7040432098765432\n",
      "linear 4 10 0.7140740740740741\n",
      "linear 4 10 0.7114506172839506\n",
      "linear 4 50 0.7040432098765432\n",
      "linear 4 50 0.7140740740740741\n",
      "linear 4 50 0.7114506172839506\n",
      "linear 4 100 0.7040432098765432\n",
      "linear 4 100 0.7140740740740741\n",
      "linear 4 100 0.7114506172839506\n",
      "linear 4 auto 0.7040432098765432\n",
      "linear 4 auto 0.7140740740740741\n",
      "linear 4 auto 0.7114506172839506\n",
      "linear 4 scale 0.7040432098765432\n",
      "linear 4 scale 0.7140740740740741\n",
      "linear 4 scale 0.7114506172839506\n",
      "linear 5 0.01 0.7039814814814814\n",
      "linear 5 0.01 0.7190123456790123\n",
      "linear 5 0.01 0.7089506172839506\n",
      "linear 5 0.1 0.7039814814814814\n",
      "linear 5 0.1 0.7190123456790123\n",
      "linear 5 0.1 0.7089506172839506\n",
      "linear 5 1 0.7039814814814814\n",
      "linear 5 1 0.7190123456790123\n",
      "linear 5 1 0.7089506172839506\n",
      "linear 5 10 0.7039814814814814\n",
      "linear 5 10 0.7190123456790123\n",
      "linear 5 10 0.7089506172839506\n",
      "linear 5 50 0.7039814814814814\n",
      "linear 5 50 0.7190123456790123\n",
      "linear 5 50 0.7089506172839506\n",
      "linear 5 100 0.7039814814814814\n",
      "linear 5 100 0.7190123456790123\n",
      "linear 5 100 0.7089506172839506\n",
      "linear 5 auto 0.7039814814814814\n",
      "linear 5 auto 0.7190123456790123\n",
      "linear 5 auto 0.7089506172839506\n",
      "linear 5 scale 0.7039814814814814\n",
      "linear 5 scale 0.7190123456790123\n",
      "linear 5 scale 0.7089506172839506\n",
      "linear 6 0.01 0.7089814814814815\n",
      "linear 6 0.01 0.7190123456790123\n",
      "linear 6 0.01 0.7089506172839507\n",
      "linear 6 0.1 0.7089814814814815\n",
      "linear 6 0.1 0.7190123456790123\n",
      "linear 6 0.1 0.7089506172839507\n",
      "linear 6 1 0.7089814814814815\n",
      "linear 6 1 0.7190123456790123\n",
      "linear 6 1 0.7089506172839507\n",
      "linear 6 10 0.7089814814814815\n",
      "linear 6 10 0.7190123456790123\n",
      "linear 6 10 0.7089506172839507\n",
      "linear 6 50 0.7089814814814815\n",
      "linear 6 50 0.7190123456790123\n",
      "linear 6 50 0.7089506172839507\n",
      "linear 6 100 0.7089814814814815\n",
      "linear 6 100 0.7190123456790123\n",
      "linear 6 100 0.7089506172839507\n",
      "linear 6 auto 0.7089814814814815\n",
      "linear 6 auto 0.7190123456790123\n",
      "linear 6 auto 0.7089506172839507\n",
      "linear 6 scale 0.7089814814814815\n",
      "linear 6 scale 0.7190123456790123\n",
      "linear 6 scale 0.7089506172839507\n",
      "linear 7 0.01 0.7114506172839506\n",
      "linear 7 0.01 0.7140432098765432\n",
      "linear 7 0.01 0.7089506172839507\n",
      "linear 7 0.1 0.7114506172839506\n",
      "linear 7 0.1 0.7140432098765432\n",
      "linear 7 0.1 0.7089506172839507\n",
      "linear 7 1 0.7114506172839506\n",
      "linear 7 1 0.7140432098765432\n",
      "linear 7 1 0.7089506172839507\n",
      "linear 7 10 0.7114506172839506\n",
      "linear 7 10 0.7140432098765432\n",
      "linear 7 10 0.7089506172839507\n",
      "linear 7 50 0.7114506172839506\n",
      "linear 7 50 0.7140432098765432\n",
      "linear 7 50 0.7089506172839507\n",
      "linear 7 100 0.7114506172839506\n",
      "linear 7 100 0.7140432098765432\n",
      "linear 7 100 0.7089506172839507\n",
      "linear 7 auto 0.7114506172839506\n",
      "linear 7 auto 0.7140432098765432\n",
      "linear 7 auto 0.7089506172839507\n",
      "linear 7 scale 0.7114506172839506\n",
      "linear 7 scale 0.7140432098765432\n",
      "linear 7 scale 0.7089506172839507\n",
      "linear 8 0.01 0.7139506172839507\n",
      "linear 8 0.01 0.7140432098765432\n",
      "linear 8 0.01 0.7064506172839506\n",
      "linear 8 0.1 0.7139506172839507\n",
      "linear 8 0.1 0.7140432098765432\n",
      "linear 8 0.1 0.7064506172839506\n",
      "linear 8 1 0.7139506172839507\n",
      "linear 8 1 0.7140432098765432\n",
      "linear 8 1 0.7064506172839506\n",
      "linear 8 10 0.7139506172839507\n",
      "linear 8 10 0.7140432098765432\n",
      "linear 8 10 0.7064506172839506\n",
      "linear 8 50 0.7139506172839507\n",
      "linear 8 50 0.7140432098765432\n",
      "linear 8 50 0.7064506172839506\n",
      "linear 8 100 0.7139506172839507\n",
      "linear 8 100 0.7140432098765432\n",
      "linear 8 100 0.7064506172839506\n",
      "linear 8 auto 0.7139506172839507\n",
      "linear 8 auto 0.7140432098765432\n",
      "linear 8 auto 0.7064506172839506\n",
      "linear 8 scale 0.7139506172839507\n",
      "linear 8 scale 0.7140432098765432\n",
      "linear 8 scale 0.7064506172839506\n",
      "poly 1 0.01 0.5173765432098765\n",
      "poly 1 0.01 0.5024691358024691\n",
      "poly 1 0.01 0.6344753086419754\n",
      "poly 1 0.1 0.6668518518518518\n",
      "poly 1 0.1 0.6594444444444445\n",
      "poly 1 0.1 0.6494444444444445\n",
      "poly 1 1 0.6270061728395062\n",
      "poly 1 1 0.6593518518518517\n",
      "poly 1 1 0.6393209876543209\n",
      "poly 1 10 0.6466358024691358\n",
      "poly 1 10 0.6392283950617283\n"
     ]
    }
   ],
   "source": [
    "for k in kernel:\n",
    "    for c in range(1, 9):\n",
    "        for g in gamma:\n",
    "            svc = SVC(kernel=k, C=(c), probability=True, gamma=g, random_state=42)\n",
    "            score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "            print(f'{k} {c} {g} {score}')\n",
    "            \n",
    "            svc = SVC(kernel=k, C=(c * .1), probability=True, gamma=g, random_state=42)\n",
    "            score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "            print(f'{k} {c} {g} {score}')\n",
    "            \n",
    "            svc = SVC(kernel=k, C=(c * 10), probability=True, gamma=g, random_state=42)\n",
    "            score = cross_val_score(svc, X, Y, cv=5, scoring='accuracy').mean()\n",
    "            print(f'{k} {c} {g} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimators = [10, 50, 100, 200, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = ['gini', 'entropy', 'log_loss']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 gini 0.6309979494190021\n",
      "10 entropy 0.6361585782638415\n",
      "10 log_loss 0.6361585782638415\n",
      "50 gini 0.6414559125085442\n",
      "50 entropy 0.6570061517429938\n",
      "50 log_loss 0.6570061517429938\n",
      "100 gini 0.638755980861244\n",
      "100 entropy 0.6466848940533151\n",
      "100 log_loss 0.6466848940533151\n",
      "200 gini 0.638755980861244\n",
      "200 entropy 0.6491797676008202\n",
      "200 log_loss 0.6491797676008202\n",
      "500 gini 0.6387559808612441\n",
      "500 entropy 0.6387901572112098\n",
      "500 log_loss 0.6387901572112098\n"
     ]
    }
   ],
   "source": [
    "for ne in n_estimators:\n",
    "    for c in criterion:\n",
    "        forest = RandomForestClassifier(n_estimators=ne, criterion=c, random_state=42)\n",
    "        score = cross_val_score(forest, X, Y, cv=5, scoring='accuracy').mean()\n",
    "        print(f'{ne} {c} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_svc = SVC(kernel='linear', C=7, probability=True, gamma=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "second_best_svc = SVC(kernel='linear', C=10, probability=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd = SGDClassifier(loss='epsilon_insensitive', penalty=None, alpha=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('svc1', best_svc), ('svc2', second_best_svc), ('sgd', sgd)],\n",
    "                              voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = cross_val_score(voting_clf, X, Y, cv=5, scoring='accuracy').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6597060833902939"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('svc1', best_svc), ('svc2', second_best_svc)],\n",
    "                              voting='hard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7146274777853725"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(voting_clf, X, Y, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voting_clf = VotingClassifier(estimators=[('svc1', best_svc), ('svc2', second_best_svc)],\n",
    "                              voting='soft')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7120300751879698"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score = cross_val_score(voting_clf, X, Y, cv=5, scoring='accuracy').mean()\n",
    "score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'epsilon_insensitive', 'squared_epsilon_insensitive']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "penalty = ['l1', 'l2', 'elasticnet', None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = [0.0001, 0.001, 0.01, 0.1, 1, 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hinge l1 0.0001 0.6543745727956254\n",
      "hinge l1 0.001 0.6911825017088175\n",
      "hinge l1 0.01 0.7043062200956938\n",
      "hinge l1 0.1 0.6937799043062202\n",
      "hinge l1 1 0.5\n",
      "hinge l1 10 0.5\n",
      "hinge l2 0.0001 0.6963089542036911\n",
      "hinge l2 0.001 0.7016062884483938\n",
      "hinge l2 0.01 0.6990088858509911\n",
      "hinge l2 0.1 0.6990088858509911\n",
      "hinge l2 1 0.6884825700615175\n",
      "hinge l2 10 0.5105263157894736\n",
      "hinge elasticnet 0.0001 0.6517088174982912\n",
      "hinge elasticnet 0.001 0.7016746411483253\n",
      "hinge elasticnet 0.01 0.7016746411483255\n",
      "hinge elasticnet 0.1 0.6912166780587834\n",
      "hinge elasticnet 1 0.6807928913192071\n",
      "hinge elasticnet 10 0.5\n",
      "hinge None 0.0001 0.6279904306220095\n",
      "hinge None 0.001 0.6911141490088859\n",
      "hinge None 0.01 0.7095010252904989\n",
      "hinge None 0.1 0.6990772385509227\n",
      "hinge None 1 0.6860218728639781\n",
      "hinge None 10 0.5969583048530417\n",
      "log_loss l1 0.0001 0.6725905673274093\n",
      "log_loss l1 0.001 0.6964114832535885\n",
      "log_loss l1 0.01 0.7042720437457279\n",
      "log_loss l1 0.1 0.6912508544087491\n",
      "log_loss l1 1 0.5\n",
      "log_loss l1 10 0.5\n",
      "log_loss l2 0.0001 0.6258714969241285\n",
      "log_loss l2 0.001 0.6781271360218729\n",
      "log_loss l2 0.01 0.7043403964456596\n",
      "log_loss l2 0.1 0.6911141490088859\n",
      "log_loss l2 1 0.6621326042378675\n",
      "log_loss l2 10 0.513157894736842\n",
      "log_loss elasticnet 0.0001 0.6336978810663021\n",
      "log_loss elasticnet 0.001 0.6964456596035544\n",
      "log_loss elasticnet 0.01 0.7069377990430622\n",
      "log_loss elasticnet 0.1 0.7042036910457963\n",
      "log_loss elasticnet 1 0.6101161995898838\n",
      "log_loss elasticnet 10 0.5\n",
      "log_loss None 0.0001 0.6440533151059468\n",
      "log_loss None 0.001 0.6884825700615174\n",
      "log_loss None 0.01 0.7095010252904989\n",
      "log_loss None 0.1 0.7043062200956938\n",
      "log_loss None 1 0.7095693779904306\n",
      "log_loss None 10 0.6179084073820916\n",
      "modified_huber l1 0.0001 0.6152426520847574\n",
      "modified_huber l1 0.001 0.6103896103896104\n",
      "modified_huber l1 0.01 0.6884825700615174\n",
      "modified_huber l1 0.1 0.6911825017088175\n",
      "modified_huber l1 1 0.5\n",
      "modified_huber l1 10 0.5\n",
      "modified_huber l2 0.0001 0.6384483937115516\n",
      "modified_huber l2 0.001 0.6545796308954205\n",
      "modified_huber l2 0.01 0.6807245386192755\n",
      "modified_huber l2 0.1 0.7043403964456596\n",
      "modified_huber l2 1 0.6884825700615174\n",
      "modified_huber l2 10 0.6074162679425836\n",
      "modified_huber elasticnet 0.0001 0.6807587149692413\n",
      "modified_huber elasticnet 0.001 0.6598427887901572\n",
      "modified_huber elasticnet 0.01 0.6911483253588517\n",
      "modified_huber elasticnet 0.1 0.699043062200957\n",
      "modified_huber elasticnet 1 0.7068010936431989\n",
      "modified_huber elasticnet 10 0.5\n",
      "modified_huber None 0.0001 0.6519480519480519\n",
      "modified_huber None 0.001 0.6703007518796993\n",
      "modified_huber None 0.01 0.6910457963089542\n",
      "modified_huber None 0.1 0.7094668489405331\n",
      "modified_huber None 1 0.7147641831852358\n",
      "modified_huber None 10 0.6361585782638415\n",
      "squared_hinge l1 0.0001 0.6490772385509228\n",
      "squared_hinge l1 0.001 0.6285372522214627\n",
      "squared_hinge l1 0.01 0.6411825017088175\n",
      "squared_hinge l1 0.1 0.641285030758715\n",
      "squared_hinge l1 1 0.5\n",
      "squared_hinge l1 10 0.5\n",
      "squared_hinge l2 0.0001 0.6125427204374573\n",
      "squared_hinge l2 0.001 0.6152426520847574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_hinge l2 0.01 0.643814080656186\n",
      "squared_hinge l2 0.1 0.6490772385509228\n",
      "squared_hinge l2 1 0.6884825700615174\n",
      "squared_hinge l2 10 0.6100478468899521\n",
      "squared_hinge elasticnet 0.0001 0.6022214627477785\n",
      "squared_hinge elasticnet 0.001 0.6360560492139439\n",
      "squared_hinge elasticnet 0.01 0.6359876965140123\n",
      "squared_hinge elasticnet 0.1 0.651742993848257\n",
      "squared_hinge elasticnet 1 0.7068010936431989\n",
      "squared_hinge elasticnet 10 0.5\n",
      "squared_hinge None 0.0001 0.6365345181134654\n",
      "squared_hinge None 0.001 0.5713943950786057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_hinge None 0.01 0.643814080656186\n",
      "squared_hinge None 0.1 0.641285030758715\n",
      "squared_hinge None 1 0.662200956937799\n",
      "squared_hinge None 10 0.649213943950786\n",
      "perceptron l1 0.0001 0.709501025290499\n",
      "perceptron l1 0.001 0.5865003417634997\n",
      "perceptron l1 0.01 0.5680109364319891\n",
      "perceptron l1 0.1 0.5\n",
      "perceptron l1 1 0.5\n",
      "perceptron l1 10 0.5\n",
      "perceptron l2 0.0001 0.5993164730006836\n",
      "perceptron l2 0.001 0.6569036226930963\n",
      "perceptron l2 0.01 0.5708475734791525\n",
      "perceptron l2 0.1 0.675393028024607\n",
      "perceptron l2 1 0.6756664388243335\n",
      "perceptron l2 10 0.5340738209159261\n",
      "perceptron elasticnet 0.0001 0.6305536568694463\n",
      "perceptron elasticnet 0.001 0.6070745044429255\n",
      "perceptron elasticnet 0.01 0.6676691729323307\n",
      "perceptron elasticnet 0.1 0.49736842105263157\n",
      "perceptron elasticnet 1 0.5236842105263158\n",
      "perceptron elasticnet 10 0.5\n",
      "perceptron None 0.0001 0.6596377306903622\n",
      "perceptron None 0.001 0.6572112098427888\n",
      "perceptron None 0.01 0.6307587149692413\n",
      "perceptron None 0.1 0.607177033492823\n",
      "perceptron None 1 0.6675324675324675\n",
      "perceptron None 10 0.5786397812713602\n",
      "squared_error l1 0.0001 0.4550922761449077\n",
      "squared_error l1 0.001 0.5212918660287081\n",
      "squared_error l1 0.01 0.4945317840054682\n",
      "squared_error l1 0.1 0.5104921394395079\n",
      "squared_error l1 1 0.4974025974025974\n",
      "squared_error l1 10 0.5\n",
      "squared_error l2 0.0001 0.5002050580997949\n",
      "squared_error l2 0.001 0.5103896103896103\n",
      "squared_error l2 0.01 0.4842105263157895\n",
      "squared_error l2 0.1 0.4945317840054682\n",
      "squared_error l2 1 0.6832194121667807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_error l2 10 0.5472317156527683\n",
      "squared_error elasticnet 0.0001 0.4664046479835953\n",
      "squared_error elasticnet 0.001 0.47857142857142854\n",
      "squared_error elasticnet 0.01 0.4763841421736158\n",
      "squared_error elasticnet 0.1 0.47628161312371836\n",
      "squared_error elasticnet 1 0.7094326725905673\n",
      "squared_error elasticnet 10 0.5\n",
      "squared_error None 0.0001 0.4106288448393712\n",
      "squared_error None 0.001 0.44483937115516065\n",
      "squared_error None 0.01 0.4737525632262475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_error None 0.1 0.47365003417635\n",
      "squared_error None 1 0.7042378673957621\n",
      "squared_error None 10 0.651742993848257\n",
      "epsilon_insensitive l1 0.0001 0.4809637730690362\n",
      "epsilon_insensitive l1 0.001 0.578468899521531\n",
      "epsilon_insensitive l1 0.01 0.7094668489405331\n",
      "epsilon_insensitive l1 0.1 0.7042720437457279\n",
      "epsilon_insensitive l1 1 0.5\n",
      "epsilon_insensitive l1 10 0.5\n",
      "epsilon_insensitive l2 0.0001 0.6517088174982912\n",
      "epsilon_insensitive l2 0.001 0.6464114832535885\n",
      "epsilon_insensitive l2 0.01 0.7069377990430622\n",
      "epsilon_insensitive l2 0.1 0.7095693779904306\n",
      "epsilon_insensitive l2 1 0.685850991114149\n",
      "epsilon_insensitive l2 10 0.5131237183868762\n",
      "epsilon_insensitive elasticnet 0.0001 0.5161995898838004\n",
      "epsilon_insensitive elasticnet 0.001 0.5445317840054683\n",
      "epsilon_insensitive elasticnet 0.01 0.7120984278879015\n",
      "epsilon_insensitive elasticnet 0.1 0.7017088174982912\n",
      "epsilon_insensitive elasticnet 1 0.6885850991114149\n",
      "epsilon_insensitive elasticnet 10 0.5\n",
      "epsilon_insensitive None 0.0001 0.5286397812713602\n",
      "epsilon_insensitive None 0.001 0.6234791524265209\n",
      "epsilon_insensitive None 0.01 0.6885850991114149\n",
      "epsilon_insensitive None 0.1 0.7225563909774435\n",
      "epsilon_insensitive None 1 0.7095352016404648\n",
      "epsilon_insensitive None 10 0.6021531100478468\n",
      "squared_epsilon_insensitive l1 0.0001 0.37901572112098425\n",
      "squared_epsilon_insensitive l1 0.001 0.49736842105263157\n",
      "squared_epsilon_insensitive l1 0.01 0.5156527682843473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_epsilon_insensitive l1 0.1 0.46329460013670537\n",
      "squared_epsilon_insensitive l1 1 0.4974025974025974\n",
      "squared_epsilon_insensitive l1 10 0.5\n",
      "squared_epsilon_insensitive l2 0.0001 0.5005126452494875\n",
      "squared_epsilon_insensitive l2 0.001 0.5683868762816131\n",
      "squared_epsilon_insensitive l2 0.01 0.4660970608339029\n",
      "squared_epsilon_insensitive l2 0.1 0.45553656869446335\n",
      "squared_epsilon_insensitive l2 1 0.6911141490088859\n",
      "squared_epsilon_insensitive l2 10 0.6072795625427204\n",
      "squared_epsilon_insensitive elasticnet 0.0001 0.5808612440191387\n",
      "squared_epsilon_insensitive elasticnet 0.001 0.46336295283663703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "squared_epsilon_insensitive elasticnet 0.01 0.5001025290498975\n",
      "squared_epsilon_insensitive elasticnet 0.1 0.4398496240601504\n",
      "squared_epsilon_insensitive elasticnet 1 0.7041695146958304\n",
      "squared_epsilon_insensitive elasticnet 10 0.5\n",
      "squared_epsilon_insensitive None 0.0001 0.45269993164730005\n",
      "squared_epsilon_insensitive None 0.001 0.4265550239234449\n",
      "squared_epsilon_insensitive None 0.01 0.4947368421052632\n",
      "squared_epsilon_insensitive None 0.1 0.46332877648667126\n",
      "squared_epsilon_insensitive None 1 0.6441558441558441\n",
      "squared_epsilon_insensitive None 10 0.6597060833902939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "/Users/sun/anaconda3/envs/senior/lib/python3.12/site-packages/sklearn/linear_model/_stochastic_gradient.py:713: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "for l in loss:\n",
    "    for p in penalty:\n",
    "        for a in alpha:\n",
    "            sgd = SGDClassifier(loss=l, penalty=p, alpha=a, random_state=42)\n",
    "            score = cross_val_score(sgd, X, Y, cv=5, scoring='accuracy').mean()\n",
    "            print(f'{l} {p} {a} {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "senior",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
